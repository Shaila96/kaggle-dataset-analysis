{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Lemmatization - Portuguese\n",
    "\n",
    "\n",
    "https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html#3\n",
    "\n",
    "\n",
    "https://spacy.io/models/pt#pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "\n",
    "# python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "text = \"\"\n",
    "pos = \"\"\n",
    "lemma = \"\"\n",
    "for token in nlp(\"Estás bem ?\"):\n",
    "    text += token.text + \"\\t\"\n",
    "    pos += token.pos_ + \"\\t\"\n",
    "    lemma += token.lemma_ + \"\\t\"\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás\tbem\t?\t\n",
      "Estás\tbem\t?\t\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install stanza\n",
    "# stanza.download('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 5.65MB/s]                    \n",
      "2020-11-21 18:14:04 INFO: Downloading default packages for language: pt (Portuguese)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/pt/default.zip: 100%|██████████| 227M/227M [04:19<00:00, 873kB/s]  \n",
      "2020-11-21 18:18:28 INFO: Finished downloading models and saved to /Users/shailazaman/stanza_resources.\n",
      "2020-11-21 18:18:28 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "| depparse  | bosque  |\n",
      "=======================\n",
      "\n",
      "2020-11-21 18:18:28 INFO: Use device: cpu\n",
      "2020-11-21 18:18:28 INFO: Loading: tokenize\n",
      "2020-11-21 18:18:28 INFO: Loading: mwt\n",
      "2020-11-21 18:18:28 INFO: Loading: pos\n",
      "2020-11-21 18:18:29 INFO: Loading: lemma\n",
      "2020-11-21 18:18:29 INFO: Loading: depparse\n",
      "2020-11-21 18:18:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não\t,\tminha\tmiúda\tem\to\tsentido\tque\tés\tcomo\tuma\tirmã\tpara\tmim\t.\t\n",
      "ADV\tPUNCT\tDET\tNOUN\tADP\tDET\tNOUN\tPRON\tAUX\tADP\tDET\tNOUN\tADP\tPRON\tPUNCT\t\n",
      "não\t,\tmeu\tmiúda\tem\to\tsentido\tque\tser\tcomo\tum\tirmã\tpara\teu\t.\t\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('pt')\n",
    "nlp = stanza.Pipeline('pt')\n",
    "\n",
    "text = \"\"\n",
    "pos = \"\"\n",
    "lemma = \"\"\n",
    "for sent in nlp(\"Não, minha miúda no sentido que és como uma irmã para mim.\").sentences:\n",
    "    for word in sent.words:\n",
    "        text += word.text + \"\\t\"\n",
    "        pos += word.upos + \"\\t\"\n",
    "        lemma += word.lemma + \"\\t\"\n",
    "\n",
    "print(text)\n",
    "print(pos)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
